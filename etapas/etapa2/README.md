# Etapa 2: Pr√©-Processamento de Dados

## üéØ Objetivos

Nesta etapa, seu objetivo √© limpar e transformar os dados brutos que voc√™ analisou na Etapa 1, preparando-os para serem usados em um modelo de Machine Learning. Voc√™ ir√° aplicar t√©cnicas para tratar problemas de qualidade e para converter os dados em um formato num√©rico e padronizado.

---

## üìù Tarefas Principais

Seu trabalho ser√° documentado em um notebook Jupyter (`notebooks/02_Preprocessamento.ipynb`). Organize seu notebook seguindo as se√ß√µes abaixo. Para cada tarefa, escreva o c√≥digo necess√°rio e, em seguida, use c√©lulas de Markdown para documentar suas decis√µes.

### 1. Tratamento de Valores Faltantes

**Por que √© importante?** A maioria dos algoritmos de Machine Learning n√£o consegue lidar com valores ausentes (`NaN`). Deix√°-los no dataset resultar√° em erros.

**O que fazer:**
- **Para colunas num√©ricas:** Preencha os valores faltantes usando uma estrat√©gia de imputa√ß√£o.
  - **Como fazer (dicas):** Use o `SimpleImputer` do Scikit-learn ou o m√©todo `.fillna()` do Pandas.
  - **Estrat√©gia `median` (mediana):** Mais segura e recomendada se a coluna tiver outliers ou uma distribui√ß√£o assim√©trica.
  - **Estrat√©gia `mean` (m√©dia):** Funciona bem para colunas com distribui√ß√£o sim√©trica (semelhante a um sino).
- **Para colunas categ√≥ricas:** Preencha os valores faltantes com a categoria mais comum.
  - **Como fazer (dicas):** Use a estrat√©gia `most_frequent` (moda) do `SimpleImputer` ou o m√©todo `.fillna(df['coluna'].mode()[0])`.

‚ö†Ô∏è **Cuidado com Data Leakage:** Para evitar vazar informa√ß√£o dos dados de teste para o treino, o ideal √© que qualquer c√°lculo (como m√©dia ou mediana) seja feito **apenas** com os dados de treino e depois aplicado aos dados de teste. Em um primeiro momento, voc√™ pode fazer no dataset todo para simplificar, mas tenha essa boa pr√°tica em mente para a Etapa 3.

---

### 2. Tratamento de Outliers

**Por que √© importante?** Valores extremos (outliers) podem distorcer a escala das features e influenciar negativamente o treinamento de alguns modelos, especialmente os lineares.

**O que fazer:**
- Com base na sua an√°lise da Etapa 1, decida como tratar os outliers em colunas num√©ricas importantes.
- **Como fazer (dicas):**
  - **Remo√ß√£o:** Se voc√™ tem certeza de que o outlier √© um erro de medi√ß√£o ou digita√ß√£o e eles s√£o poucos, voc√™ pode remov√™-los. Use com cuidado para n√£o perder dados valiosos. Ex: `df = df[df['coluna'] < valor_maximo]`.
  - **Capping (Limitar):** Uma abordagem mais segura √© "aparar" os outliers, substituindo-os por um valor m√°ximo ou m√≠nimo aceit√°vel (por exemplo, o limite do "whisker" do boxplot, que √© `Q3 + 1.5 * IQR`).
  - **Manter:** Se os outliers s√£o valores raros, mas leg√≠timos (ex: uma venda de valor muito alto em um e-commerce), pode ser melhor mant√™-los. Transforma√ß√µes (como a de log) podem ajudar a reduzir seu impacto.

---

### 3. Encoding de Vari√°veis Categ√≥ricas

**Por que √© importante?** Modelos de ML trabalham com n√∫meros, n√£o com texto. Precisamos converter colunas categ√≥ricas (como "g√™nero" ou "cidade") em um formato num√©rico.

**O que fazer:**
- Converta todas as colunas de texto (tipo `object`) em representa√ß√µes num√©ricas.
- **Como fazer (dicas):**
  - **One-Hot Encoding (para vari√°veis nominais):** Use esta t√©cnica para colunas onde as categorias **n√£o t√™m uma ordem** natural (ex: `cidade`, `cor_favorita`). Ela cria novas colunas bin√°rias (0 ou 1) para cada categoria.
    - **Ferramenta:** `pd.get_dummies(df, columns=['coluna_a', 'coluna_b'], drop_first=True)`.
    - O `drop_first=True` √© importante para remover uma das categorias, evitando redund√¢ncia de informa√ß√£o (multicolinearidade).
  - **Label Encoding / Ordinal Encoding (para vari√°veis ordinais):** Use para colunas onde as categorias **t√™m uma ordem** clara (ex: `ruim`, `m√©dio`, `bom`). Ela atribui um n√∫mero inteiro a cada categoria (ex: 0, 1, 2), preservando a ordem.
    - **Ferramenta:** `OrdinalEncoder` do Scikit-learn ou o m√©todo `.map({'ruim': 0, 'm√©dio': 1, 'bom': 2})` do Pandas.

---

### 4. Normaliza√ß√£o de Vari√°veis Num√©ricas

**Por que √© importante?** Features com escalas muito diferentes (ex: `idade` variando de 18 a 70 e `sal√°rio` variando de 1.000 a 100.000) podem fazer com que o modelo d√™ mais import√¢ncia √† feature com a escala maior. A normaliza√ß√£o coloca todas na mesma escala.

**O que fazer:**
- Aplique uma t√©cnica de scaling a todas as suas colunas num√©ricas (ap√≥s tratar outliers e faltantes).
- **Como fazer (dicas):**
  - **StandardScaler:** Transforma os dados para que tenham m√©dia 0 e desvio padr√£o 1. √â a t√©cnica mais comum e funciona bem para a maioria dos algoritmos.
  - **MinMaxScaler:** Transforma os dados para que fiquem em um intervalo espec√≠fico, geralmente entre 0 e 1.
- **Salve o scaler:** Ap√≥s treinar o scaler (`scaler.fit(dados_de_treino)`), √© **crucial** salv√°-lo em um arquivo.
  - **Ferramenta:** `import joblib; joblib.dump(scaler, 'models/scaler.pkl')`.
  - Isso garante que voc√™ poder√° aplicar **exatamente a mesma transforma√ß√£o** nos dados de teste e em novos dados no futuro.

---

### 5. Feature Engineering (Opcional)

**Por que √© importante?** √Äs vezes, as colunas originais n√£o cont√™m toda a informa√ß√£o. Criar novas features pode ajudar o modelo a encontrar padr√µes que n√£o eram √≥bvios antes.

**O que fazer:**
- Crie 1 ou 2 novas colunas a partir das existentes que voc√™ acredita que possam ser √∫teis.
- **Como fazer (dicas de ideias):**
  - **Criar uma raz√£o:** Se voc√™ tem `distancia_km` e `tempo_minutos`, pode criar `velocidade_media = distancia_km / (tempo_minutos / 60)`.
  - **Combinar features:** Se voc√™ tem `numero_de_filhos` e `estado_civil`, pode criar uma feature bin√°ria `tem_familia_grande`.
  - **Extrair de datas:** Se tiver uma coluna de data, pode extrair o dia da semana, o m√™s ou se √© um fim de semana.

---

## üìä Entreg√°veis

1.  **Notebook (`notebooks/02_Preprocessamento.ipynb`):** Contendo todo o c√≥digo e as justificativas em Markdown para suas decis√µes.
2.  **Dataset Limpo (`data/students_clean.csv`):** O DataFrame final, ap√≥s todas as transforma√ß√µes, salvo em um novo arquivo CSV.
3.  **Scaler Salvo (`models/scaler.pkl`):** O objeto do scaler treinado e salvo com `joblib`.

---

## üé§ Apresenta√ß√£o (5 minutos)

Prepare uma apresenta√ß√£o curta e objetiva (4-5 slides) para resumir seu trabalho.

- **Slide 1: Resumo dos Problemas Corrigidos:** Quantos valores faltantes foram tratados? Quantos outliers foram identificados e o que voc√™ fez com eles?
- **Slide 2: Principais Transforma√ß√µes:** Mostre um exemplo de encoding (One-Hot ou Label) e como ficou o resultado. Mostre um gr√°fico de uma vari√°vel antes e depois da normaliza√ß√£o.
- **Slide 3: Feature Engineering (se aplic√°vel):** Apresente a(s) nova(s) feature(s) que voc√™ criou e por que acredita que ela(s) pode(m) ser √∫til(is).
- **Slide 4: Resultado Final:** Mostre as dimens√µes do dataset antes e depois (`X` linhas, `Y` colunas -> `X` linhas, `Z` colunas) e declare que os dados est√£o prontos para a modelagem.

---

## ‚úÖ Checklist de Sucesso

- [ ] Seu notebook est√° organizado com t√≠tulos para cada tarefa de pr√©-processamento.
- [ ] Todas as decis√µes (ex: por que usou mediana, por que removeu outliers) est√£o justificadas em Markdown.
- [ ] O dataset limpo foi salvo corretamente em `data/students_clean.csv`.
- [ ] O scaler foi salvo em `models/scaler.pkl`.
- [ ] O notebook executa do in√≠cio ao fim sem erros.

**Bom trabalho!** üöÄ